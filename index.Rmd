---
title: "Musical ecstasy"
author: "Maryse Sturm"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
---

### introduction: 'High on music'

I've been raised in a (protestant) christian household, and have gone to churches for as long as i can remember. Sometimes I think I've seen it all, from the traditional 'sit down' churches with the wooden benches and enourmous organs, to the evangelical, almost concert-like, gatherings and everything in between. As you can imagine, music is a big part of almost every sunday-service I've ever attended, each current with its own signature genre. What stands out to me is the way christians experience some kind of ecstasy, especially in the more evangelical part of the church. The music, to me, seems designed for a state of trance and worship, although I could never really say what things specifically contribute to that.

In this corpus I want to focus on the ecstasy people experience when listening to music. Specifically, worship music. I want to compare this to another genre which is generally known for its transcendental tendencies, something people listen to when they want to fall into ecstasy. So, for this comparison I chose dance music. I am aware of the extend of these 2 genres, both worship and dance music contain many subgenres and so many different 'vibes', so I carefully chose 2 playlists (to stay objective, I did not use my own playlist of worship songs, since I did not have such a playlist with dance songs). These playlists both promise in their description to get you in a state of trance, and create an euphoric atmosphere. I wonder if there are some common musical factors able to create such an atmosphere, or if there is nothing to see at all. 

***



<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DXbtYAdenGE9U?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>


<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DX5pEiFLSS7sX?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

### comparing playlists: energy and valance

```{R}

library(ggplot2)
library(spotifyr)
library(tidyverse)
library(compmus)

worship <- get_playlist_audio_features("", "37i9dQZF1DX5pEiFLSS7sX")
dance <- get_playlist_audio_features("", "37i9dQZF1DXbtYAdenGE9U")

comparing_playlists <-
  bind_rows(
    worship %>% mutate(category = "worship"),
    dance %>% mutate(category = "dance")
  )
comparing_playlists %>%                    
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     
    aes(
      x = valence,
      y = energy,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +     
  geom_rug(size = 0.1) +    
  geom_text(                  
    aes(
      x = valence,
      y = energy,
      label = label
    ),
    data = 
      tibble(
        label = c(""),
        category = c("worship", "dance"),
        valence = c(0.090, 0.123),
        energy = c(0.101, 0.967)
      ),
    colour = "black",         
    size = 3,                 
    hjust = "left",           
    vjust = "bottom",         
    nudge_x = -0.05,          
    nudge_y = 0.02            
  ) +
  facet_wrap(~category) +     
  scale_x_continuous(        
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   
    minor_breaks = NULL       
  ) +
  scale_y_continuous(         
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        
    type = "qual",            
    palette = "Reds" 
  ) +
  scale_size_continuous(      
    trans = "exp",            
    guide = "none"            
  ) +
  theme_light() +             
  labs(                       
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  )
```

*** 
There are a lot of things you can conclude only out of these 2 graphs, one of them being the obvious difference in the distribution of energy through the playlists. As you maybe can expect, the energy-levels in the dance-playlist are very high, the lowest still above .5, and everything else just below 1.0. In the worship-playlist 

### tempogram

```{R}
library(tidyverse)
library(compmus)
library(spotifyr)

fearless <- get_tidy_audio_analysis("5rp9r3rFr45bPTsgi18tQB")

fearless %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***
This tempogram came out great, really steady with a straight line at around 120 bpm. I was surprised, because this is a live take on the song, so I think it is neat that the line is straight like this. Not perfect, but clear enough for a live performance. 


### keygrams and chordograms

```{R}
library(tidyverse)
library(spotifyr)
library(compmus)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

new_wine <-
  get_tidy_audio_analysis("1MX4dx6yNvAH2GjIVsUF4h") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

new_wine %>% 
  compmus_match_pitch_template(
    key_templates,         
    method = "euclidean",  
    norm = "manhattan"     
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


***
The key of this piece is BbMaj. Since its related key is Gminor, it is not surprising to see that a few sections in this chordogram also turn dark-blue. When I tried to do a chordogram on beats instead of sections you could see more clearly that most of the piece is definately in Bbmaj, but for the sake of a clear and easy-to-read graph I chose 'sections' in this chordogram. At one point (just before 300 seconds) you can see a yellow line go all the way down, indicating there is no clear key at that point. Confusing, you might think, but when you listen to the song it suddenly is obvious: there is a big round of applause (it is a live take of the song). 



### chromagrams part one: worship

```{R}
library(tidyverse)
library(spotifyr)
library(compmus)

NIHOT <- get_tidy_audio_analysis("2orHAZyBJH5rbojtwPsztm") %>% select(segments) %>% unnest(segments) %>% select(start, duration, pitches)

NIHOT %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

***


### chromagram part two: dance

```{R}
library(tidyverse)
library(spotifyr)
library(compmus)

eightynine <- get_tidy_audio_analysis("1ZuCmG8sXSqgMzpnlOonPv") %>% select(segments) %>% unnest(segments) %>% select(start, duration, pitches)

eightynine %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

```
*** 


### self-similarity matrices 

```{R}
library(tidyverse)
library(spotifyr)
library(compmus)

bzt <- get_tidy_audio_analysis("2soQW3y7DwZIPH4fS5av1W") %>% 
  compmus_align(bars, segments) %>%                     
  select(bars) %>%                                      
  unnest(bars) %>%                                      
  mutate(pitches = map(segments,compmus_summarise, pitches,
                       method = "rms", norm = "euclidean")) %>%
  mutate(
    timbre =
      map(segments,
          compmus_summarise, timbre,
          method = "rms", norm = "euclidean"))

bzt %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")

```

*** 




